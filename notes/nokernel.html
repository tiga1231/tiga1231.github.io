<html>
<head>
<style>
table, th, td {
   border: 1px solid black;
}
table {
    border-collapse: collapse;
}
th {
    text-align: left;
}
col{
    width: 50px;
}
</style>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({ tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
</script>
<script async src='https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'></script>
</head>
<body>
<h1>There is no kernel</h1>
<!--h2>Term description</h2>
<p>In the context of machine learning,</p>
<dl>
<dt>Kernel of two vectors</dt>
<dd>Dot product of two vectors $v_i, v_j$ in a vector space $V$ of features.</dd>
<dt>Kernel matrix</dt>
<dd>Given $n$ vectors $v_i \in V$, entries in the kernel matrix are dot products of each two of the vectors. i.e. kernel matrix stores the pairwise dot products.</dd>
<dt></dt>
<dd></dd>
</dl-->

<h2>Statement</h2>
<p>Having an $n\times n$ kernel matrix is <em>equivalent</em> to having <em>some</em> feature space of dimension k (k $\leq$ n, is the rank of the matrix), up to change of basis. </p>
<h2>Explanation</h2>
<p>For concreteness, suppose we are given n vectors (n data points) in a particular vector space, say $\mathbb{R}^3$, recorded in a table. e.g</p>
<table>
<col><col><col><col>
<tr><th>i</th><th>x</th><th>y</th><th>z</th></tr>
<tr><td>1</td><td>3.4</td><td>5.3</td><td>2.1</td></tr>
<tr><td>2</td><td>6.1</td><td>0.0</td><td>2.6</td></tr>
<tr><td>3</td><td>0.2</td><td>1.5</td><td>0.0</td></tr>
<tr><td>4</td><td>2.3</td><td>1.0</td><td>0.5</td></tr>
<tr><td>$\vdots$</td><td>$\vdots$</td><td>$\vdots$</td><td>$\vdots$</td><tr>
<tr><td>i</td><td>$x_i$</td><td>y_i</td><td>z_i</td></tr>
<tr><td>$\vdots$</td><td>$\vdots$</td><td>$\vdots$</td><td>$\vdots$</td><tr>
<tr><td>n</td><td>$x_n$</td><td>y_n</td><td>z_n</td></tr>
</table>
<p>Note that this is indeed a table of dot products between each vector $v_i$ with $u_1 = (1,0,0), u_2=(0,1,0)$, and $u_3=(0,0,1)$</p>
<p>If we choose 3 linearly independent vectors from our dataset $\{v_1, v_2, \cdots, v_n\}$, say 
$$u_1'= v_1 = (3.4, 5.3, 2.1) $$
$$u_2'= v_2 = (6.1, 0.0, 2.6) $$
$$u_3'= v_3 = (0.2, 1,5, 0.0) $$
These three vectors spans $\mathbb{R}^3$ as well as $\{(1,0,0), (0,1,0), (0,0,1)\}$ did, thus can be treated as a new basis. If we compute the dot products of all data vectors with respect to each of these, we get a set of new coordinates of the same data. In this case:
</p>
<table>
<col><col><col><col>
<tr><th>i</th><th>$u_1$</th><th>$u_2$</th><th>$u_3$</th></tr>
<tr><td>1</td><td>44.06</td><td>26.2</td><td>8.63</td></tr>
<tr><td>2</td><td>26.2</td><td>43.97</td><td>1.22</td></tr>
<tr><td>3</td><td>8.63</td><td>1.22</td><td>2.29</td></tr>
<tr><td>4</td><td>14.17</td><td>15.33</td><td>1.96</td></tr>
<tr><td>$\vdots$</td><td>$\vdots$</td><td>$\vdots$</td><td>$\vdots$</td><tr>
<tr><td>i</td>
<td>$\langle x_i, u_1 \rangle$</td>
<td>$\langle y_i, u_2 \rangle$</td>
<td>$\langle z_i, u_3 \rangle$</td>
</tr>
<tr><td>$\vdots$</td><td>$\vdots$</td><td>$\vdots$</td><td>$\vdots$</td><tr>
<tr><td>n</td>
<td>$\langle x_n, u_1 \rangle$</td>
<td>$\langle y_n, u_2 \rangle$</td>
<td>$\langle z_n, u_3 \rangle$</td>
</tr>
</table>
<p>We have a new coordiante system that behaves similar to the original one, with a difference that it is scaled, rotated and sheared.</p>
<p>If we add more redundant information, i.e more dot products with vectors in $\{v_1, v_2, \cdots, v_n\}$, we are still in $\mathbb{R}^3$, but now embeded, as a subspace in $\mathbb{R^n}$</p>
<p>This is why vectors in kernel matrices work the same as normal feature vectors that we may not have access to. As machine learning techniques do not require the orthogonality or unit length of the basis, we only need to define a notion of dot products. And <em>more importantly</em>, we don't have to (and shouldn't) treat the kernel matrix different from data features we normally obtain, feeding them into any existing learning system (PCA, least squares) that operates on normal feature spaces just work.</p>
<!--p>The kernel normally have less interpretability, though.</p-->
</body>
</html>
